Index: linux-3.10.0-123.el7.x86_64_patch/fs/btrfs/btrfs_inode.h
===================================================================
--- linux-3.10.0-123.el7.x86_64_patch.orig/fs/btrfs/btrfs_inode.h
+++ linux-3.10.0-123.el7.x86_64_patch/fs/btrfs/btrfs_inode.h
@@ -322,4 +322,21 @@ void btrfs_add_transaction_callback(stru
 				    struct btrfs_trans_cb_entry *callback);
 bool btrfs_try_del_transaction_callback(struct btrfs_trans_handle *trans,
 					struct btrfs_trans_cb_entry *callback);
+int btrfs_update_inode(struct btrfs_trans_handle *trans,
+		       struct btrfs_root *root, struct inode *inode);
+struct inode *btrfs_create_inode(struct btrfs_trans_handle *trans,
+				 struct inode *dir, umode_t mode,
+				 dev_t rdev, u64 *index);
+int btrfs_add_entry(struct btrfs_trans_handle *trans,
+		    struct inode *dir,
+		    struct inode *inode,
+		    struct qstr *d_name,
+		    u64 *index);
+int btrfs_punch(struct inode *inode, struct btrfs_trans_handle *trans,
+		__u64 start, __u64 end);
+int btrfs_read_prep(struct inode *inode, struct page **pages, int nr_pages);
+int btrfs_write_commit(struct inode *inode, struct page **pages, int nr_pages);
+int __btrfs_setxattr(struct btrfs_trans_handle *trans,
+		     struct inode *inode, const char *name,
+		     const void *value, size_t size, int flags);
 #endif
Index: linux-3.10.0-123.el7.x86_64_patch/fs/btrfs/inode.c
===================================================================
--- linux-3.10.0-123.el7.x86_64_patch.orig/fs/btrfs/inode.c
+++ linux-3.10.0-123.el7.x86_64_patch/fs/btrfs/inode.c
@@ -88,13 +88,14 @@ static int btrfs_finish_ordered_io(struc
 static noinline int cow_file_range(struct inode *inode,
 				   struct page *locked_page,
 				   u64 start, u64 end, int *page_started,
-				   unsigned long *nr_written, int unlock);
+				   unsigned long *nr_written, int unlock,
+				   struct page **locked_pages,
+				   int locked_page_num);
 static struct extent_map *create_pinned_em(struct inode *inode, u64 start,
 					   u64 len, u64 orig_start,
 					   u64 block_start, u64 block_len,
 					   u64 orig_block_len, u64 ram_bytes,
 					   int type);
-
 static int btrfs_dirty_inode(struct inode *inode);
 
 int btrfs_init_inode_security(struct btrfs_trans_handle *trans,
@@ -296,6 +297,8 @@ struct async_cow {
 	u64 end;
 	struct list_head extents;
 	struct btrfs_work work;
+	struct page **locked_pages;
+	int locked_page_num;
 };
 
 static noinline int add_async_extent(struct async_cow *cow,
@@ -340,7 +343,9 @@ static noinline int compress_file_range(
 					struct page *locked_page,
 					u64 start, u64 end,
 					struct async_cow *async_cow,
-					int *num_added)
+					int *num_added,
+					struct page **locked_pages,
+					int locked_page_num)
 {
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	u64 num_bytes;
@@ -487,7 +492,8 @@ cont:
 						     clear_flags, PAGE_UNLOCK |
 						     PAGE_CLEAR_DIRTY |
 						     PAGE_SET_WRITEBACK |
-						     PAGE_END_WRITEBACK);
+						     PAGE_END_WRITEBACK,
+						     locked_pages, locked_page_num);
 			goto free_pages_out;
 		}
 	}
@@ -626,7 +632,9 @@ retry:
 					     async_extent->start,
 					     async_extent->start +
 					     async_extent->ram_size - 1,
-					     &page_started, &nr_written, 0);
+					     &page_started, &nr_written, 0,
+					     async_cow->locked_pages,
+					     async_cow->locked_page_num);
 
 			/* JDM XXX */
 
@@ -740,7 +748,9 @@ retry:
 				async_extent->ram_size - 1,
 				NULL, EXTENT_LOCKED | EXTENT_DELALLOC,
 				PAGE_UNLOCK | PAGE_CLEAR_DIRTY |
-				PAGE_SET_WRITEBACK);
+				PAGE_SET_WRITEBACK,
+				async_cow->locked_pages,
+				async_cow->locked_page_num);
 		ret = btrfs_submit_compressed_write(inode,
 				    async_extent->start,
 				    async_extent->ram_size,
@@ -765,7 +775,9 @@ out_free:
 				     NULL, EXTENT_LOCKED | EXTENT_DELALLOC |
 				     EXTENT_DEFRAG | EXTENT_DO_ACCOUNTING,
 				     PAGE_UNLOCK | PAGE_CLEAR_DIRTY |
-				     PAGE_SET_WRITEBACK | PAGE_END_WRITEBACK);
+				     PAGE_SET_WRITEBACK | PAGE_END_WRITEBACK,
+				     async_cow->locked_pages,
+				     async_cow->locked_page_num);
 	kfree(async_extent);
 	goto again;
 }
@@ -819,7 +831,9 @@ static noinline int cow_file_range(struc
 				   struct page *locked_page,
 				   u64 start, u64 end, int *page_started,
 				   unsigned long *nr_written,
-				   int unlock)
+				   int unlock,
+				   struct page **locked_pages,
+				   int locked_page_num)
 {
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	u64 alloc_hint = 0;
@@ -856,7 +870,8 @@ static noinline int cow_file_range(struc
 				     EXTENT_LOCKED | EXTENT_DELALLOC |
 				     EXTENT_DEFRAG, PAGE_UNLOCK |
 				     PAGE_CLEAR_DIRTY | PAGE_SET_WRITEBACK |
-				     PAGE_END_WRITEBACK);
+				     PAGE_END_WRITEBACK,
+				     locked_pages, locked_page_num);
 
 			*nr_written = *nr_written +
 			     (end - start + PAGE_CACHE_SIZE) / PAGE_CACHE_SIZE;
@@ -947,7 +962,8 @@ static noinline int cow_file_range(struc
 		extent_clear_unlock_delalloc(inode, start,
 					     start + ram_size - 1, locked_page,
 					     EXTENT_LOCKED | EXTENT_DELALLOC,
-					     op);
+					     op, locked_pages,
+					     locked_page_num);
 		disk_num_bytes -= cur_alloc_size;
 		num_bytes -= cur_alloc_size;
 		alloc_hint = ins.objectid + ins.offset;
@@ -963,7 +979,8 @@ out_unlock:
 				     EXTENT_LOCKED | EXTENT_DO_ACCOUNTING |
 				     EXTENT_DELALLOC | EXTENT_DEFRAG,
 				     PAGE_UNLOCK | PAGE_CLEAR_DIRTY |
-				     PAGE_SET_WRITEBACK | PAGE_END_WRITEBACK);
+				     PAGE_SET_WRITEBACK | PAGE_END_WRITEBACK,
+				     locked_pages, locked_page_num);
 	goto out;
 }
 
@@ -978,7 +995,8 @@ static noinline void async_cow_start(str
 
 	compress_file_range(async_cow->inode, async_cow->locked_page,
 			    async_cow->start, async_cow->end, async_cow,
-			    &num_added);
+			    &num_added, async_cow->locked_pages,
+			    async_cow->locked_page_num);
 	if (num_added == 0) {
 		btrfs_add_delayed_iput(async_cow->inode);
 		async_cow->inode = NULL;
@@ -1020,7 +1038,9 @@ static noinline void async_cow_free(stru
 
 static int cow_file_range_async(struct inode *inode, struct page *locked_page,
 				u64 start, u64 end, int *page_started,
-				unsigned long *nr_written)
+				unsigned long *nr_written,
+				struct page **locked_pages,
+				int locked_page_num)
 {
 	struct async_cow *async_cow;
 	struct btrfs_root *root = BTRFS_I(inode)->root;
@@ -1037,6 +1057,8 @@ static int cow_file_range_async(struct i
 		async_cow->root = root;
 		async_cow->locked_page = locked_page;
 		async_cow->start = start;
+		async_cow->locked_pages = locked_pages;
+		async_cow->locked_page_num = locked_page_num;
 
 		if (BTRFS_I(inode)->flags & BTRFS_INODE_NOCOMPRESS)
 			cur_end = end;
@@ -1107,8 +1129,10 @@ static noinline int csum_exist_in_range(
  */
 static noinline int run_delalloc_nocow(struct inode *inode,
 				       struct page *locked_page,
-			      u64 start, u64 end, int *page_started, int force,
-			      unsigned long *nr_written)
+				       u64 start, u64 end, int *page_started,
+				       int force, unsigned long *nr_written,
+				       struct page **locked_pages,
+				       int locked_page_num)
 {
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct btrfs_trans_handle *trans;
@@ -1140,7 +1164,8 @@ static noinline int run_delalloc_nocow(s
 					     EXTENT_DEFRAG, PAGE_UNLOCK |
 					     PAGE_CLEAR_DIRTY |
 					     PAGE_SET_WRITEBACK |
-					     PAGE_END_WRITEBACK);
+					     PAGE_END_WRITEBACK,
+					     locked_pages, locked_page_num);
 		return -ENOMEM;
 	}
 
@@ -1158,7 +1183,8 @@ static noinline int run_delalloc_nocow(s
 					     EXTENT_DEFRAG, PAGE_UNLOCK |
 					     PAGE_CLEAR_DIRTY |
 					     PAGE_SET_WRITEBACK |
-					     PAGE_END_WRITEBACK);
+					     PAGE_END_WRITEBACK,
+					     locked_pages, locked_page_num);
 		btrfs_free_path(path);
 		return PTR_ERR(trans);
 	}
@@ -1276,7 +1302,8 @@ out_check:
 		if (cow_start != (u64)-1) {
 			ret = cow_file_range(inode, locked_page,
 					     cow_start, found_key.offset - 1,
-					     page_started, nr_written, 1);
+					     page_started, nr_written, 1,
+					     locked_pages, locked_page_num);
 			if (ret)
 				goto error;
 			cow_start = (u64)-1;
@@ -1333,7 +1360,8 @@ out_check:
 					     cur_offset + num_bytes - 1,
 					     locked_page, EXTENT_LOCKED |
 					     EXTENT_DELALLOC, PAGE_UNLOCK |
-					     PAGE_SET_PRIVATE2);
+					     PAGE_SET_PRIVATE2,
+					     locked_pages, locked_page_num);
 		cur_offset = extent_end;
 		if (cur_offset > end)
 			break;
@@ -1347,7 +1375,8 @@ out_check:
 
 	if (cow_start != (u64)-1) {
 		ret = cow_file_range(inode, locked_page, cow_start, end,
-				     page_started, nr_written, 1);
+				     page_started, nr_written, 1,
+				     NULL, 0);
 		if (ret)
 			goto error;
 	}
@@ -1364,7 +1393,8 @@ error:
 					     EXTENT_DO_ACCOUNTING, PAGE_UNLOCK |
 					     PAGE_CLEAR_DIRTY |
 					     PAGE_SET_WRITEBACK |
-					     PAGE_END_WRITEBACK);
+					     PAGE_END_WRITEBACK,
+					     locked_pages, locked_page_num);
 	btrfs_free_path(path);
 	return ret;
 }
@@ -1374,27 +1404,34 @@ error:
  */
 static int run_delalloc_range(struct inode *inode, struct page *locked_page,
 			      u64 start, u64 end, int *page_started,
-			      unsigned long *nr_written)
+			      unsigned long *nr_written,
+			      struct page **locked_pages, int locked_page_num)
 {
 	int ret;
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 
 	if (BTRFS_I(inode)->flags & BTRFS_INODE_NODATACOW) {
 		ret = run_delalloc_nocow(inode, locked_page, start, end,
-					 page_started, 1, nr_written);
+					 page_started, 1, nr_written,
+					 locked_pages, locked_page_num);
 	} else if (BTRFS_I(inode)->flags & BTRFS_INODE_PREALLOC) {
 		ret = run_delalloc_nocow(inode, locked_page, start, end,
-					 page_started, 0, nr_written);
+					 page_started, 0, nr_written,
+					 locked_pages, locked_page_num);
 	} else if (!btrfs_test_opt(root, COMPRESS) &&
 		   !(BTRFS_I(inode)->force_compress) &&
 		   !(BTRFS_I(inode)->flags & BTRFS_INODE_COMPRESS)) {
 		ret = cow_file_range(inode, locked_page, start, end,
-				      page_started, nr_written, 1);
+				     page_started, nr_written, 1,
+				     locked_pages, locked_page_num);
 	} else {
 		set_bit(BTRFS_INODE_HAS_ASYNC_EXTENT,
 			&BTRFS_I(inode)->runtime_flags);
+		/* LIXI TODO: fix async unlock */
+		BUG_ON(locked_page_num);
 		ret = cow_file_range_async(inode, locked_page, start, end,
-					   page_started, nr_written);
+					   page_started, nr_written,
+					   locked_pages, locked_page_num);
 	}
 	return ret;
 }
@@ -3527,6 +3564,8 @@ noinline int btrfs_update_inode(struct b
 
 	return btrfs_update_inode_item(trans, root, inode);
 }
+EXPORT_SYMBOL(btrfs_update_inode);
+
 
 noinline int btrfs_update_inode_fallback(struct btrfs_trans_handle *trans,
 					 struct btrfs_root *root,
@@ -3639,6 +3678,7 @@ int btrfs_unlink_inode(struct btrfs_tran
 	}
 	return ret;
 }
+EXPORT_SYMBOL(btrfs_unlink_inode);
 
 /*
  * helper to start transaction for unlink and rmdir.
@@ -4942,6 +4982,7 @@ struct inode *btrfs_lookup_dentry(struct
 
 	return inode;
 }
+EXPORT_SYMBOL(btrfs_lookup_dentry);
 
 static int btrfs_dentry_delete(const struct dentry *dentry)
 {
@@ -5358,6 +5399,7 @@ struct inode *btrfs_new_inode(struct btr
 	struct btrfs_inode_ref *ref;
 	struct btrfs_key key[2];
 	u32 sizes[2];
+	int nitems = name ? 2 : 1;
 	unsigned long ptr;
 	int ret;
 	int owner;
@@ -5378,7 +5420,7 @@ struct inode *btrfs_new_inode(struct btr
 	 */
 	inode->i_ino = objectid;
 
-	if (dir) {
+	if (dir && name) {
 		trace_btrfs_inode_request(dir);
 
 		ret = btrfs_set_inode_index(dir, index);
@@ -5387,6 +5429,8 @@ struct inode *btrfs_new_inode(struct btr
 			iput(inode);
 			return ERR_PTR(ret);
 		}
+	} else if (dir) {
+		*index = 0;
 	}
 	/*
 	 * index_cnt is ignored for everything but a dir,
@@ -5415,21 +5459,24 @@ struct inode *btrfs_new_inode(struct btr
 	btrfs_set_key_type(&key[0], BTRFS_INODE_ITEM_KEY);
 	key[0].offset = 0;
 
-	/*
-	 * Start new inodes with an inode_ref. This is slightly more
-	 * efficient for small numbers of hard links since they will
-	 * be packed into one item. Extended refs will kick in if we
-	 * add more hard links than can fit in the ref item.
-	 */
-	key[1].objectid = objectid;
-	btrfs_set_key_type(&key[1], BTRFS_INODE_REF_KEY);
-	key[1].offset = ref_objectid;
-
 	sizes[0] = sizeof(struct btrfs_inode_item);
-	sizes[1] = name_len + sizeof(*ref);
+
+	if (name) {
+		/*
+		 * Start new inodes with an inode_ref. This is slightly more
+		 * efficient for small numbers of hard links since they will
+		 * be packed into one item. Extended refs will kick in if we
+		 * add more hard links than can fit in the ref item.
+		 */
+		key[1].objectid = objectid;
+		btrfs_set_key_type(&key[1], BTRFS_INODE_REF_KEY);
+		key[1].offset = ref_objectid;
+
+		sizes[1] = name_len + sizeof(*ref);
+	}
 
 	path->leave_spinning = 1;
-	ret = btrfs_insert_empty_items(trans, root, path, key, sizes, 2);
+	ret = btrfs_insert_empty_items(trans, root, path, key, sizes, nitems);
 	if (ret != 0)
 		goto fail;
 
@@ -5442,12 +5489,14 @@ struct inode *btrfs_new_inode(struct btr
 			     sizeof(*inode_item));
 	fill_inode_item(trans, path->nodes[0], inode_item, inode);
 
-	ref = btrfs_item_ptr(path->nodes[0], path->slots[0] + 1,
-			     struct btrfs_inode_ref);
-	btrfs_set_inode_ref_name_len(path->nodes[0], ref, name_len);
-	btrfs_set_inode_ref_index(path->nodes[0], ref, *index);
-	ptr = (unsigned long)(ref + 1);
-	write_extent_buffer(path->nodes[0], name, ptr, name_len);
+	if (name) {
+		ref = btrfs_item_ptr(path->nodes[0], path->slots[0] + 1,
+				     struct btrfs_inode_ref);
+		btrfs_set_inode_ref_name_len(path->nodes[0], ref, name_len);
+		btrfs_set_inode_ref_index(path->nodes[0], ref, *index);
+		ptr = (unsigned long)(ref + 1);
+		write_extent_buffer(path->nodes[0], name, ptr, name_len);
+	}
 
 	btrfs_mark_buffer_dirty(path->nodes[0]);
 	btrfs_free_path(path);
@@ -8639,6 +8688,349 @@ static int btrfs_permission(struct inode
 	return generic_permission(inode, mask);
 }
 
+struct inode *btrfs_create_inode(struct btrfs_trans_handle *trans,
+				 struct inode *dir, umode_t mode,
+				 dev_t rdev,
+				 u64 *index)
+{
+	struct btrfs_root *root = BTRFS_I(dir)->root;
+	struct inode *inode;
+	u64 objectid;
+	int err;
+
+	err = btrfs_find_free_ino(root, &objectid);
+	if (err)
+		return ERR_PTR(err);
+
+	inode = btrfs_new_inode(trans, root, dir, NULL,
+				0, btrfs_ino(dir), objectid,
+				mode, index);
+	if (IS_ERR(inode))
+		return inode;
+
+	btrfs_i_size_write(inode, 0);
+
+	err = btrfs_update_inode(trans, root, inode);
+	if (err)
+		goto out_drop;
+
+	switch (mode & S_IFMT) {
+	case S_IFREG:
+		inode->i_fop = &btrfs_file_operations;
+		inode->i_op = &btrfs_file_inode_operations;
+		inode->i_mapping->a_ops = &btrfs_aops;
+		inode->i_mapping->backing_dev_info = &root->fs_info->bdi;
+		BTRFS_I(inode)->io_tree.ops = &btrfs_extent_io_ops;
+		break;
+	case S_IFDIR:
+		inode->i_fop = &btrfs_dir_file_operations;
+		inode->i_op = &btrfs_dir_inode_operations;
+		break;
+	case S_IFLNK:
+		inode->i_fop = &btrfs_file_operations;
+		inode->i_op = &btrfs_symlink_inode_operations;
+		inode->i_mapping->a_ops = &btrfs_symlink_aops;
+		inode->i_mapping->backing_dev_info = &root->fs_info->bdi;
+		BTRFS_I(inode)->io_tree.ops = &btrfs_extent_io_ops;
+		break;
+	case S_IFBLK:
+	case S_IFCHR:
+	case S_IFIFO:
+	case S_IFSOCK:
+		inode->i_op = &btrfs_special_inode_operations;
+
+		init_special_inode(inode, inode->i_mode, rdev);
+		break;
+	default:
+		err = -EINVAL;
+		goto out_drop;
+		break;
+	}
+	return inode;
+out_drop:
+	inode_dec_link_count(inode);
+	iput(inode);
+	return ERR_PTR(err);
+}
+EXPORT_SYMBOL(btrfs_create_inode);
+
+static int __btrfs_add_entry(struct btrfs_trans_handle *trans,
+			     struct inode *dir,
+			     struct inode *inode,
+			     const char *name, int name_len,
+			     u64 *index)
+{
+	int ret;
+	struct btrfs_root *root = BTRFS_I(dir)->root;
+	struct btrfs_path *path;
+	struct btrfs_inode_ref *ref;
+	struct btrfs_key key;
+	u64 objectid = BTRFS_I(inode)->location.objectid;
+	u64 ref_objectid = btrfs_ino(dir);
+	u32 size;
+	unsigned long ptr;
+
+	path = btrfs_alloc_path();
+	if (!path)
+		return -ENOMEM;
+
+	trace_btrfs_inode_request(dir);
+	ret = btrfs_set_inode_index(dir, index);
+	if (ret) {
+		btrfs_free_path(path);
+		return ret;
+	}
+
+#ifdef LIXI
+	/* Need this for master branch of btrfs */
+	BTRFS_I(inode)->dir_index = *index;
+#endif /* LIXI */
+
+	/*
+	 * Start new inodes with an inode_ref. This is slightly more
+	 * efficient for small numbers of hard links since they will
+	 * be packed into one item. Extended refs will kick in if we
+	 * add more hard links than can fit in the ref item.
+	 */
+	key.objectid = objectid;
+	btrfs_set_key_type(&key, BTRFS_INODE_REF_KEY);
+	key.offset = ref_objectid;
+	size = name_len + sizeof(*ref);
+
+	path->leave_spinning = 1;
+	ret = btrfs_insert_empty_items(trans, root, path, &key, &size, 1);
+	if (ret != 0)
+		goto fail;
+
+	ref = btrfs_item_ptr(path->nodes[0], path->slots[0],
+			     struct btrfs_inode_ref);
+	btrfs_set_inode_ref_name_len(path->nodes[0], ref, name_len);
+	btrfs_set_inode_ref_index(path->nodes[0], ref, *index);
+	ptr = (unsigned long)(ref + 1);
+	write_extent_buffer(path->nodes[0], name, ptr, name_len);
+
+	btrfs_mark_buffer_dirty(path->nodes[0]);
+	btrfs_free_path(path);
+
+	return 0;
+fail:
+	BTRFS_I(dir)->index_cnt--;
+	btrfs_free_path(path);
+	return ret;
+}
+
+int btrfs_add_entry(struct btrfs_trans_handle *trans,
+		    struct inode *dir,
+		    struct inode *inode,
+		    struct qstr *d_name,
+		    u64 *index)
+{
+	int err;
+	struct btrfs_root *root = BTRFS_I(dir)->root;
+
+	err = __btrfs_add_entry(trans, dir, inode,
+				d_name->name, d_name->len,
+				index);
+	if (err)
+		return err;
+
+	err = btrfs_init_inode_security(trans, inode, dir, d_name);
+	if (err)
+		return err;
+
+	err = btrfs_add_link(trans, dir, inode, d_name->name,
+			     d_name->len, 0, *index);
+	if (err)
+		return err;
+
+	err = btrfs_update_inode(trans, root, inode);
+	if (err)
+		return err;
+
+	btrfs_balance_delayed_items(root);
+	btrfs_btree_balance_dirty(root);
+	return 0;
+}
+EXPORT_SYMBOL(btrfs_add_entry);
+
+static int _btrfs_punch(struct inode *inode, struct btrfs_trans_handle *trans)
+{
+	struct btrfs_root *root = BTRFS_I(inode)->root;
+	struct btrfs_block_rsv *rsv;
+	int ret = 0;
+	u64 mask = root->sectorsize - 1;
+	u64 min_size = btrfs_calc_trunc_metadata_size(root, 1);
+
+	ret = btrfs_wait_ordered_range(inode, inode->i_size & (~mask),
+				       (u64)-1);
+	if (ret)
+		return ret;
+
+	/*
+	 * Yes ladies and gentelment, this is indeed ugly.  The fact is we have
+	 * 3 things going on here
+	 *
+	 * 1) We need to reserve space for our orphan item and the space to
+	 * delete our orphan item.  Lord knows we don't want to have a dangling
+	 * orphan item because we didn't reserve space to remove it.
+	 *
+	 * 2) We need to reserve space to update our inode.
+	 *
+	 * 3) We need to have something to cache all the space that is going to
+	 * be free'd up by the truncate operation, but also have some slack
+	 * space reserved in case it uses space during the truncate (thank you
+	 * very much snapshotting).
+	 *
+	 * And we need these to all be seperate.  The fact is we can use alot of
+	 * space doing the truncate, and we have no earthly idea how much space
+	 * we will use, so we need the truncate reservation to be seperate so it
+	 * doesn't end up using space reserved for updating the inode or
+	 * removing the orphan item.  We also need to be able to stop the
+	 * transaction and start a new one, which means we need to be able to
+	 * update the inode several times, and we have no idea of knowing how
+	 * many times that will be, so we can't just reserve 1 item for the
+	 * entirety of the opration, so that has to be done seperately as well.
+	 * Then there is the orphan item, which does indeed need to be held on
+	 * to for the whole operation, and we need nobody to touch this reserved
+	 * space except the orphan code.
+	 *
+	 * So that leaves us with
+	 *
+	 * 1) root->orphan_block_rsv - for the orphan deletion.
+	 * 2) rsv - for the truncate reservation, which we will steal from the
+	 * transaction reservation.
+	 * 3) fs_info->trans_block_rsv - this will have 1 items worth left for
+	 * updating the inode.
+	 */
+	rsv = btrfs_alloc_block_rsv(root, BTRFS_BLOCK_RSV_TEMP);
+	if (!rsv)
+		return -ENOMEM;
+	rsv->size = min_size;
+	rsv->failfast = 1;
+
+	/* Migrate the slack space for the truncate to our reserve */
+	ret = btrfs_block_rsv_migrate(&root->fs_info->trans_block_rsv, rsv,
+				      min_size);
+	BUG_ON(ret);
+
+	/*
+	 * setattr is responsible for setting the ordered_data_close flag,
+	 * but that is only tested during the last file release.  That
+	 * could happen well after the next commit, leaving a great big
+	 * window where new writes may get lost if someone chooses to write
+	 * to this file after truncating to zero
+	 *
+	 * The inode doesn't have any dirty data here, and so if we commit
+	 * this is a noop.  If someone immediately starts writing to the inode
+	 * it is very likely we'll catch some of their writes in this
+	 * transaction, and the commit will find this file on the ordered
+	 * data list with good things to send down.
+	 *
+	 * This is a best effort solution, there is still a window where
+	 * using truncate to replace the contents of the file will
+	 * end up with a zero length file after a crash.
+	 */
+	if (inode->i_size == 0 && test_bit(BTRFS_INODE_ORDERED_DATA_CLOSE,
+					   &BTRFS_I(inode)->runtime_flags))
+		btrfs_add_ordered_operation(trans, root, inode);
+
+	/*
+	 * So if we truncate and then write and fsync we normally would just
+	 * write the extents that changed, which is a problem if we need to
+	 * first truncate that entire inode.  So set this flag so we write out
+	 * all of the extents in the inode to the sync log so we're completely
+	 * safe.
+	 */
+	set_bit(BTRFS_INODE_NEEDS_FULL_SYNC, &BTRFS_I(inode)->runtime_flags);
+	trans->block_rsv = rsv;
+
+	while (1) {
+		ret = btrfs_truncate_inode_items(trans, root, inode,
+						 inode->i_size,
+						 BTRFS_EXTENT_DATA_KEY);
+		if (ret != -ENOSPC) {
+			break;
+		}
+
+		trans->block_rsv = &root->fs_info->trans_block_rsv;
+		ret = btrfs_update_inode(trans, root, inode);
+		if (ret) {
+			break;
+		}
+
+		btrfs_btree_balance_dirty(root);
+
+		ret = btrfs_block_rsv_migrate(&root->fs_info->trans_block_rsv,
+					      rsv, min_size);
+		BUG_ON(ret);	/* shouldn't happen */
+		trans->block_rsv = rsv;
+	}
+
+	if (ret == 0 && inode->i_nlink > 0) {
+		trans->block_rsv = root->orphan_block_rsv;
+		btrfs_orphan_del(trans, inode);
+	}
+
+	trans->block_rsv = &root->fs_info->trans_block_rsv;
+	ret = btrfs_update_inode(trans, root, inode);
+
+	btrfs_btree_balance_dirty(root);
+
+	btrfs_free_block_rsv(root, rsv);
+
+	return ret;
+}
+
+int btrfs_punch(struct inode *inode, struct btrfs_trans_handle *trans,
+		__u64 start, __u64 end)
+{
+	int ret;
+
+	BUG_ON(end != 0xffffffffffffffffULL);
+
+	/*
+	 * We're truncating a file that used to have good data down to
+	 * zero. Make sure it gets into the ordered flush list so that
+	 * any new writes get down to disk quickly.
+	 */
+	if (start == 0)
+		set_bit(BTRFS_INODE_ORDERED_DATA_CLOSE,
+			&BTRFS_I(inode)->runtime_flags);
+
+	/*
+	 * We need to do this in case we fail at _any_ point during the
+	 * actual truncate.  Once we do the truncate_setsize we could
+	 * invalidate pages which forces any outstanding ordered io to
+	 * be instantly completed which will give us extents that need
+	 * to be truncated.  If we fail to get an orphan inode down we
+	 * could have left over extents that were never meant to live,
+	 * so we need to garuntee from this point on that everything
+	 * will be consistent.
+	 */
+	ret = btrfs_orphan_add(trans, inode);
+
+	/* we don't support swapfiles, so vmtruncate shouldn't fail */
+	truncate_setsize(inode, start);
+	/* Disable nonlocked read DIO to avoid the end less truncate */
+	btrfs_inode_block_unlocked_dio(inode);
+	inode_dio_wait(inode);
+	btrfs_inode_resume_unlocked_dio(inode);
+
+	ret = _btrfs_punch(inode, trans);
+	if (ret && inode->i_nlink) {
+		/*
+		 * failed to truncate, disk_i_size is only adjusted down
+		 * as we remove extents, so it should represent the true
+		 * size of the inode, so reset the in memory size and
+		 * delete our orphan entry.
+		 */
+		i_size_write(inode, BTRFS_I(inode)->disk_i_size);
+		ret = btrfs_orphan_del(trans, inode);
+	}
+	return ret;
+}
+EXPORT_SYMBOL(btrfs_punch);
+
 const struct inode_operations btrfs_dir_inode_operations = {
 	.getattr	= btrfs_getattr,
 	.lookup		= btrfs_lookup,
Index: linux-3.10.0-123.el7.x86_64_patch/fs/btrfs/extent_io.c
===================================================================
--- linux-3.10.0-123.el7.x86_64_patch.orig/fs/btrfs/extent_io.c
+++ linux-3.10.0-123.el7.x86_64_patch/fs/btrfs/extent_io.c
@@ -1500,9 +1500,24 @@ out:
 	return found;
 }
 
+static inline int page_is_locked(struct page **locked_pages,
+				 int locked_page_num,
+				 struct page *page)
+{
+	int i;
+
+	for (i = 0; i < locked_page_num; i++) {
+		if (locked_pages[i] == page)
+			return 1;
+	}
+	return 0;
+}
+
 static noinline void __unlock_for_delalloc(struct inode *inode,
 					   struct page *locked_page,
-					   u64 start, u64 end)
+					   u64 start, u64 end,
+					   struct page **locked_pages,
+					   int locked_page_num)
 {
 	int ret;
 	struct page *pages[16];
@@ -1519,8 +1534,11 @@ static noinline void __unlock_for_delall
 				     min_t(unsigned long, nr_pages,
 				     ARRAY_SIZE(pages)), pages);
 		for (i = 0; i < ret; i++) {
-			if (pages[i] != locked_page)
-				unlock_page(pages[i]);
+			if (!page_is_locked(locked_pages, locked_page_num,
+					    pages[i])) {
+				if (pages[i] != locked_page)
+					unlock_page(pages[i]);
+			}
 			page_cache_release(pages[i]);
 		}
 		nr_pages -= ret;
@@ -1532,7 +1550,9 @@ static noinline void __unlock_for_delall
 static noinline int lock_delalloc_pages(struct inode *inode,
 					struct page *locked_page,
 					u64 delalloc_start,
-					u64 delalloc_end)
+					u64 delalloc_end,
+					struct page **locked_pages,
+					int locked_page_num)
 {
 	unsigned long index = delalloc_start >> PAGE_CACHE_SHIFT;
 	unsigned long start_index = index;
@@ -1563,18 +1583,21 @@ static noinline int lock_delalloc_pages(
 			 * the caller is taking responsibility for
 			 * locked_page
 			 */
-			if (pages[i] != locked_page) {
-				lock_page(pages[i]);
-				if (!PageDirty(pages[i]) ||
-				    pages[i]->mapping != inode->i_mapping) {
-					ret = -EAGAIN;
-					unlock_page(pages[i]);
-					page_cache_release(pages[i]);
-					goto done;
+			if (!page_is_locked(locked_pages, locked_page_num,
+					    pages[i])) {
+				if (pages[i] != locked_page) {
+					lock_page(pages[i]);
+					if (!PageDirty(pages[i]) ||
+					    pages[i]->mapping != inode->i_mapping) {
+						ret = -EAGAIN;
+						unlock_page(pages[i]);
+						page_cache_release(pages[i]);
+						goto done;
+					}
 				}
+				pages_locked++;
 			}
 			page_cache_release(pages[i]);
-			pages_locked++;
 		}
 		nrpages -= ret;
 		index += ret;
@@ -1586,7 +1609,8 @@ done:
 		__unlock_for_delalloc(inode, locked_page,
 			      delalloc_start,
 			      ((u64)(start_index + pages_locked - 1)) <<
-			      PAGE_CACHE_SHIFT);
+			      PAGE_CACHE_SHIFT,
+			      locked_pages, locked_page_num);
 	}
 	return ret;
 }
@@ -1600,7 +1624,9 @@ done:
 STATIC u64 find_lock_delalloc_range(struct inode *inode,
 				    struct extent_io_tree *tree,
 				    struct page *locked_page, u64 *start,
-				    u64 *end, u64 max_bytes)
+				    u64 *end, u64 max_bytes,
+				    struct page **locked_pages,
+				    int locked_page_num)
 {
 	u64 delalloc_start;
 	u64 delalloc_end;
@@ -1638,7 +1664,8 @@ again:
 
 	/* step two, lock all the pages after the page that has start */
 	ret = lock_delalloc_pages(inode, locked_page,
-				  delalloc_start, delalloc_end);
+				  delalloc_start, delalloc_end, locked_pages,
+				  locked_page_num);
 	if (ret == -EAGAIN) {
 		/* some of the pages are gone, lets avoid looping by
 		 * shortening the size of the delalloc range we're searching
@@ -1665,7 +1692,8 @@ again:
 		unlock_extent_cached(tree, delalloc_start, delalloc_end,
 				     &cached_state, GFP_NOFS);
 		__unlock_for_delalloc(inode, locked_page,
-			      delalloc_start, delalloc_end);
+			      delalloc_start, delalloc_end,
+			      locked_pages, locked_page_num);
 		cond_resched();
 		goto again;
 	}
@@ -1679,7 +1707,9 @@ out_failed:
 int extent_clear_unlock_delalloc(struct inode *inode, u64 start, u64 end,
 				 struct page *locked_page,
 				 unsigned long clear_bits,
-				 unsigned long page_ops)
+				 unsigned long page_ops,
+				 struct page **locked_pages,
+				 int locked_page_num)
 {
 	struct extent_io_tree *tree = &BTRFS_I(inode)->io_tree;
 	int ret;
@@ -1712,8 +1742,11 @@ int extent_clear_unlock_delalloc(struct
 				set_page_writeback(pages[i]);
 			if (page_ops & PAGE_END_WRITEBACK)
 				end_page_writeback(pages[i]);
-			if (page_ops & PAGE_UNLOCK)
-				unlock_page(pages[i]);
+			if (!page_is_locked(locked_pages,
+					    locked_page_num, pages[i])) {
+				if (page_ops & PAGE_UNLOCK)
+					unlock_page(pages[i]);
+			}
 			page_cache_release(pages[i]);
 		}
 		nr_pages -= ret;
@@ -2739,7 +2772,8 @@ static int __do_readpage(struct extent_i
 			 get_extent_t *get_extent,
 			 struct extent_map **em_cached,
 			 struct bio **bio, int mirror_num,
-			 unsigned long *bio_flags, int rw)
+			 unsigned long *bio_flags, int rw,
+			 int keep_page)
 {
 	struct inode *inode = page->mapping->host;
 	u64 start = page_offset(page);
@@ -2902,7 +2936,8 @@ out:
 	if (!nr) {
 		if (!PageError(page))
 			SetPageUptodate(page);
-		unlock_page(page);
+		if (!keep_page)
+			unlock_page(page);
 	}
 	return 0;
 }
@@ -2913,7 +2948,8 @@ static inline void __do_contiguous_readp
 					     get_extent_t *get_extent,
 					     struct extent_map **em_cached,
 					     struct bio **bio, int mirror_num,
-					     unsigned long *bio_flags, int rw)
+					     unsigned long *bio_flags, int rw,
+					     int keep_page)
 {
 	struct inode *inode;
 	struct btrfs_ordered_extent *ordered;
@@ -2933,8 +2969,9 @@ static inline void __do_contiguous_readp
 
 	for (index = 0; index < nr_pages; index++) {
 		__do_readpage(tree, pages[index], get_extent, em_cached, bio,
-			      mirror_num, bio_flags, rw);
-		page_cache_release(pages[index]);
+			      mirror_num, bio_flags, rw, keep_page);
+		if (!keep_page)
+			page_cache_release(pages[index]);
 	}
 }
 
@@ -2943,7 +2980,8 @@ static void __extent_readpages(struct ex
 			       int nr_pages, get_extent_t *get_extent,
 			       struct extent_map **em_cached,
 			       struct bio **bio, int mirror_num,
-			       unsigned long *bio_flags, int rw)
+			       unsigned long *bio_flags, int rw,
+			       int keep_page)
 {
 	u64 start = 0;
 	u64 end = 0;
@@ -2964,7 +3002,7 @@ static void __extent_readpages(struct ex
 						  index - first_index, start,
 						  end, get_extent, em_cached,
 						  bio, mirror_num, bio_flags,
-						  rw);
+						  rw, keep_page);
 			start = page_start;
 			end = start + PAGE_CACHE_SIZE - 1;
 			first_index = index;
@@ -2975,7 +3013,7 @@ static void __extent_readpages(struct ex
 		__do_contiguous_readpages(tree, &pages[first_index],
 					  index - first_index, start,
 					  end, get_extent, em_cached, bio,
-					  mirror_num, bio_flags, rw);
+					  mirror_num, bio_flags, rw, keep_page);
 }
 
 static int __extent_read_full_page(struct extent_io_tree *tree,
@@ -3001,7 +3039,7 @@ static int __extent_read_full_page(struc
 	}
 
 	ret = __do_readpage(tree, page, get_extent, NULL, bio, mirror_num,
-			    bio_flags, rw);
+			    bio_flags, rw, 0);
 	return ret;
 }
 
@@ -3027,7 +3065,7 @@ int extent_read_full_page_nolock(struct
 	int ret;
 
 	ret = __do_readpage(tree, page, get_extent, NULL, &bio, mirror_num,
-				      &bio_flags, READ);
+				      &bio_flags, READ, 0);
 	if (bio)
 		ret = submit_one_bio(READ, bio, mirror_num, bio_flags);
 	return ret;
@@ -3050,7 +3088,8 @@ static noinline void update_nr_written(s
  * and the end_io handler clears the writeback ranges
  */
 static int __extent_writepage(struct page *page, struct writeback_control *wbc,
-			      void *data)
+			      void *data, int keep_page,
+			      struct page **locked_pages, int locked_page_num)
 {
 	struct inode *inode = page->mapping->host;
 	struct extent_page_data *epd = data;
@@ -3093,24 +3132,26 @@ static int __extent_writepage(struct pag
 
 	ClearPageError(page);
 
-	pg_offset = i_size & (PAGE_CACHE_SIZE - 1);
-	if (page->index > end_index ||
-	   (page->index == end_index && !pg_offset)) {
-		page->mapping->a_ops->invalidatepage(page, 0);
-		unlock_page(page);
-		return 0;
-	}
+	if (!keep_page) {
+		pg_offset = i_size & (PAGE_CACHE_SIZE - 1);
+		if (page->index > end_index ||
+		   (page->index == end_index && !pg_offset)) {
+			page->mapping->a_ops->invalidatepage(page, 0);
+				unlock_page(page);
+			return 0;
+		}
 
-	if (page->index == end_index) {
-		char *userpage;
+		if (page->index == end_index) {
+			char *userpage;
 
-		userpage = kmap_atomic(page);
-		memset(userpage + pg_offset, 0,
-		       PAGE_CACHE_SIZE - pg_offset);
-		kunmap_atomic(userpage);
-		flush_dcache_page(page);
+			userpage = kmap_atomic(page);
+			memset(userpage + pg_offset, 0,
+			       PAGE_CACHE_SIZE - pg_offset);
+			kunmap_atomic(userpage);
+			flush_dcache_page(page);
+		}
+		pg_offset = 0;
 	}
-	pg_offset = 0;
 
 	set_page_extent_mapped(page);
 
@@ -3133,7 +3174,9 @@ static int __extent_writepage(struct pag
 						       page,
 						       &delalloc_start,
 						       &delalloc_end,
-						       128 * 1024 * 1024);
+						       128 * 1024 * 1024,
+						       locked_pages,
+						       locked_page_num);
 			if (nr_delalloc == 0) {
 				delalloc_start = delalloc_end + 1;
 				continue;
@@ -3142,7 +3185,9 @@ static int __extent_writepage(struct pag
 						       delalloc_start,
 						       delalloc_end,
 						       &page_started,
-						       &nr_written);
+						       &nr_written,
+						       locked_pages,
+						       locked_page_num);
 			/* File system has been set read-only */
 			if (ret) {
 				SetPageError(page);
@@ -3191,12 +3236,14 @@ static int __extent_writepage(struct pag
 			else
 				redirty_page_for_writepage(wbc, page);
 			update_nr_written(page, wbc, nr_written);
-			unlock_page(page);
+			if (!keep_page)
+				unlock_page(page);
 			ret = 0;
 			goto done_unlocked;
 		}
 	}
 
+
 	/*
 	 * we don't want to touch the inode after unlocking the page,
 	 * so we update the mapping writeback index now
@@ -3310,7 +3357,8 @@ done:
 		set_page_writeback(page);
 		end_page_writeback(page);
 	}
-	unlock_page(page);
+	if (!keep_page)
+		unlock_page(page);
 
 done_unlocked:
 
@@ -3785,7 +3833,7 @@ int extent_write_full_page(struct extent
 		.bio_flags = 0,
 	};
 
-	ret = __extent_writepage(page, wbc, &epd);
+	ret = __extent_writepage(page, wbc, &epd, 0, NULL, 0);
 
 	flush_epd_write_bio(&epd);
 	return ret;
@@ -3819,7 +3867,7 @@ int extent_write_locked_range(struct ext
 	while (start <= end) {
 		page = find_get_page(mapping, start >> PAGE_CACHE_SHIFT);
 		if (clear_page_dirty_for_io(page))
-			ret = __extent_writepage(page, &wbc_writepages, &epd);
+			ret = __extent_writepage(page, &wbc_writepages, &epd, 0, NULL, 0);
 		else {
 			if (tree->ops && tree->ops->writepage_end_io_hook)
 				tree->ops->writepage_end_io_hook(page, start,
@@ -3835,6 +3883,12 @@ int extent_write_locked_range(struct ext
 	return ret;
 }
 
+static int extent_writepage(struct page *page, struct writeback_control *wbc,
+			    void *data)
+{
+	return __extent_writepage(page, wbc, data, 0, NULL, 0);
+}
+
 int extent_writepages(struct extent_io_tree *tree,
 		      struct address_space *mapping,
 		      get_extent_t *get_extent,
@@ -3851,7 +3905,7 @@ int extent_writepages(struct extent_io_t
 	};
 
 	ret = extent_write_cache_pages(tree, mapping, wbc,
-				       __extent_writepage, &epd,
+				       extent_writepage, &epd,
 				       flush_write_bio);
 	flush_epd_write_bio(&epd);
 	return ret;
@@ -3885,12 +3939,12 @@ int extent_readpages(struct extent_io_tr
 		if (nr < ARRAY_SIZE(pagepool))
 			continue;
 		__extent_readpages(tree, pagepool, nr, get_extent, &em_cached,
-				   &bio, 0, &bio_flags, READ);
+				   &bio, 0, &bio_flags, READ, 0);
 		nr = 0;
 	}
 	if (nr)
 		__extent_readpages(tree, pagepool, nr, get_extent, &em_cached,
-				   &bio, 0, &bio_flags, READ);
+				   &bio, 0, &bio_flags, READ, 0);
 
 	if (em_cached)
 		free_extent_map(em_cached);
@@ -5235,3 +5289,56 @@ int try_release_extent_buffer(struct pag
 
 	return release_extent_buffer(eb);
 }
+
+int btrfs_read_prep(struct inode *inode, struct page **pages, int nr_pages)
+{
+	struct extent_map *em_cached = NULL;
+	struct bio *bio = NULL;
+	unsigned long bio_flags = 0;
+
+	__extent_readpages(&BTRFS_I(inode)->io_tree,
+			   pages, nr_pages, btrfs_get_extent, &em_cached,
+			   &bio, 0, &bio_flags, READ, 1);
+
+	if (em_cached)
+		free_extent_map(em_cached);
+
+	if (bio)
+		return submit_one_bio(READ, bio, 0, bio_flags);
+	return 0;
+}
+EXPORT_SYMBOL(btrfs_read_prep);
+
+int btrfs_write_commit(struct inode *inode, struct page **pages, int nr_pages)//,
+//		       loff_t pos, size_t write_bytes)
+{
+	struct extent_io_tree *tree = &BTRFS_I(inode)->io_tree;
+	struct writeback_control wbc = {
+		.sync_mode = WB_SYNC_NONE,
+		.nr_to_write = LONG_MAX,
+	};
+	struct extent_page_data epd = {
+		.bio = NULL,
+		.tree = tree,
+		.get_extent = btrfs_get_extent,
+		.extent_locked = 0,
+		.sync_io = 0,
+		.bio_flags = 0,
+	};
+	int i;
+	int ret = 0;
+
+	for (i = 0; i < nr_pages; i++) {
+		/* The page could have been commited in earlier iterations, skip it if so */
+		//if (PageWriteback(pages[i]) || !clear_page_dirty_for_io(pages[i]));
+		if(!clear_page_dirty_for_io(pages[i]))
+		 	continue;
+		ret = __extent_writepage(pages[i], &wbc, &epd, 1, pages, nr_pages);
+		if (ret)
+			break;
+	}
+
+	flush_epd_write_bio(&epd);
+	return ret;
+}
+EXPORT_SYMBOL(btrfs_write_commit);
Index: linux-3.10.0-123.el7.x86_64_patch/fs/btrfs/xattr.c
===================================================================
--- linux-3.10.0-123.el7.x86_64_patch.orig/fs/btrfs/xattr.c
+++ linux-3.10.0-123.el7.x86_64_patch/fs/btrfs/xattr.c
@@ -215,6 +215,7 @@ out:
 	btrfs_end_transaction(trans, root);
 	return ret;
 }
+EXPORT_SYMBOL(__btrfs_setxattr);
 
 ssize_t btrfs_listxattr(struct dentry *dentry, char *buffer, size_t size)
 {
Index: linux-3.10.0-123.el7.x86_64_patch/fs/btrfs/file.c
===================================================================
--- linux-3.10.0-123.el7.x86_64_patch.orig/fs/btrfs/file.c
+++ linux-3.10.0-123.el7.x86_64_patch/fs/btrfs/file.c
@@ -531,6 +531,7 @@ int btrfs_dirty_pages(struct btrfs_root
 		i_size_write(inode, end_pos);
 	return 0;
 }
+EXPORT_SYMBOL(btrfs_dirty_pages);
 
 /*
  * this drops all the extents in the cache that intersect the range
Index: linux-3.10.0-123.el7.x86_64_patch/fs/btrfs/extent-tree.c
===================================================================
--- linux-3.10.0-123.el7.x86_64_patch.orig/fs/btrfs/extent-tree.c
+++ linux-3.10.0-123.el7.x86_64_patch/fs/btrfs/extent-tree.c
@@ -3715,6 +3715,7 @@ commit_trans:
 
 	return 0;
 }
+EXPORT_SYMBOL(btrfs_check_data_free_space);
 
 /*
  * Called if we need to clear a data reservation for this inode.
@@ -3735,6 +3736,7 @@ void btrfs_free_reserved_data_space(stru
 				      data_sinfo->flags, bytes, 0);
 	spin_unlock(&data_sinfo->lock);
 }
+EXPORT_SYMBOL(btrfs_free_reserved_data_space);
 
 static void force_metadata_allocation(struct btrfs_fs_info *info)
 {
@@ -5108,6 +5110,7 @@ out_fail:
 		mutex_unlock(&BTRFS_I(inode)->delalloc_mutex);
 	return ret;
 }
+EXPORT_SYMBOL(btrfs_delalloc_reserve_metadata);
 
 /**
  * btrfs_delalloc_release_metadata - release a metadata reservation for an inode
@@ -5195,6 +5198,7 @@ void btrfs_delalloc_release_space(struct
 	btrfs_delalloc_release_metadata(inode, num_bytes);
 	btrfs_free_reserved_data_space(inode, num_bytes);
 }
+EXPORT_SYMBOL(btrfs_delalloc_release_space);
 
 static int update_block_group(struct btrfs_root *root,
 			      u64 bytenr, u64 num_bytes, int alloc)
Index: linux-3.10.0-123.el7.x86_64_patch/fs/btrfs/extent_io.h
===================================================================
--- linux-3.10.0-123.el7.x86_64_patch.orig/fs/btrfs/extent_io.h
+++ linux-3.10.0-123.el7.x86_64_patch/fs/btrfs/extent_io.h
@@ -68,7 +68,9 @@ typedef	int (extent_submit_bio_hook_t)(s
 struct extent_io_ops {
 	int (*fill_delalloc)(struct inode *inode, struct page *locked_page,
 			     u64 start, u64 end, int *page_started,
-			     unsigned long *nr_written);
+			     unsigned long *nr_written,
+			     struct page **locked_pages,
+			     int locked_page_num);
 	int (*writepage_start_hook)(struct page *page, u64 start, u64 end);
 	int (*writepage_io_hook)(struct page *page, u64 start, u64 end);
 	extent_submit_bio_hook_t *submit_bio_hook;
@@ -330,7 +332,9 @@ int extent_range_redirty_for_io(struct i
 int extent_clear_unlock_delalloc(struct inode *inode, u64 start, u64 end,
 				 struct page *locked_page,
 				 unsigned long bits_to_clear,
-				 unsigned long page_ops);
+				 unsigned long page_ops,
+				 struct page **locked_pages,
+				 int locked_page_num);
 struct bio *
 btrfs_bio_alloc(struct block_device *bdev, u64 first_sector, int nr_vecs,
 		gfp_t gfp_flags);
